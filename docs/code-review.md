# Code Review Feedback

## Findings
- **High – DB engine breaks on non-SQLite URLs**: `backend/app/db/session.py:8-13` unconditionally passes `connect_args={"check_same_thread": False}` to `create_engine`. This parameter is SQLite-only and will crash when `database_url` points to Postgres/MySQL (expected in deployed environments). Add conditional connect args based on scheme or allow SQLAlchemy defaults for non-SQLite.
- **High – Review flow cannot finish the last card**: In `frontend/src/pages/ReviewPage.jsx:68-102`, `handleNext` early-returns on the final card, so the “Finish” button and the auto-advance from self-grading never progress to the completion state. Users are stuck on the last card with no completion/reload path; `currentIndex` never reaches `deck.length` to render the “Review Complete” view.
- **Medium – Health endpoint always reports `status: "healthy"`**: `backend/app/main.py:44-69` returns HTTP 200 with `"status": "healthy"` even when DB/Ollama checks fail (errors only appear in fields). Monitoring/uptime checks will read false positives; bubble failures to status code or status field.
- **Medium – No server-side text length guard**: `settings.max_text_length` exists but `process_text` (`backend/app/services/ingest_service.py:38-74`) and `/api/sources` never enforce it. Large payloads can hit the LLM and DB unchecked, risking timeouts and high cost. Validate length server-side and return 4xx instead of relying on the UI’s 500-char hint.
- **Medium – Extraction translation required, causes 500s**: `VerifiedExtractionItem.english_gloss` is mandatory (`backend/app/schemas/extraction.py:64-75`) and the verifier copies `item.english_gloss` directly (`backend/app/services/verification_service.py:418-429`). If the LLM omits a gloss, Pydantic raises and the whole ingest fails. Consider making it optional or providing a fallback/error message per item.
- **Low – Duplicate method definition**: `GraphStore.get_item_by_id` is defined twice (`backend/app/services/graph_store.py:68-70` and `:459-469`), with the latter silently shadowing the former. This invites drift and confusion; remove the duplicate or consolidate.

## System Prompt Review (Extraction & Verification)
- **Extraction system prompt drift** (`backend/app/services/extract_service.py:59-135`): The system prompt still says “Extract up to {{max_items_per_type}} …” while the user prompt hardcodes the real limit with string interpolation. The double-brace token is never replaced, which weakens adherence to the actual cap. Also, neither prompt forbids Markdown/code fences, so “valid JSON” responses can arrive wrapped in ```json blocks and fail parsing. Tighten to (a) remove the stale token and restate the real cap, (b) require “return a single JSON object, no prose/markdown/code fences,” and (c) instruct to drop any item whose required fields would be blank instead of emitting placeholders.
- **Evidence fidelity** (`extract_service.py:88-138`): The prompts require “FULL sentence where found” but do not explicitly forbid paraphrase or placeholders like “full sentence.” Add a hard rule to copy the exact sentence substring from the provided `sentences` list and to echo it verbatim, rejecting synthesized text; this will reduce hallucination load on `TextVerifier`.
- **Canonicalization prompt looseness** (`backend/app/services/verification_service.py:140-168`): The single-item lemmatization prompt asks for “Return ONLY the lemma,” yet it uses an open-ended `generate` call and examples ending with `Lemma:` which often elicit extra tokens (“Lemma: gehen”, explanations, punctuation). Make the system prompt stricter (e.g., “Return exactly the lemma string, no punctuation/labels/quotes; if unsure, return the input surface”) or switch to `generate_json` with a fixed shape like `{"lemma": "<token>"}`.
- **Batch lemmatization safety** (`verification_service.py:198-225`): The batch prompt does not forbid markdown and does not instruct to preserve exact keys. When LLMs wrap output in code fences or reorder/deduplicate keys, mapping back to items fails. Add explicit instructions: “Respond with a JSON object only, no markdown; include every input surface as a key exactly as provided; if unsure, echo the surface form.”

## Prompt/Process Optimizations to Improve Extraction & Validation Quality
- **Tighten schema and rejection rules in the prompt**: Tell the model to skip (not emit) any item lacking a required field (surface_form, canonical, evidence.sentence, english_gloss). This avoids downstream 500s and reduces junk items the verifier must drop.
- **Constrain JSON formatting hard**: Add “Return exactly one JSON object, no markdown/code fences, no trailing text. If unable, return `{"items":[],"edges":[],"sentences":[]}`.” This reduces parse failures and retries.
- **Ground evidence explicitly**: Provide the split sentence list in the prompt and require the model to copy the exact sentence text; instruct “do not paraphrase/shorten; if unsure, omit the item.” Pair with a check that `sentence_idx` must point to one of the provided sentences.
- **Reinforce German-specific canonical rules**: Inline concise rules for separable verbs and reflexives (e.g., “rufe … an” → “anrufen”; “sich kennengelernt” → “sich kennenlernen”), and ask for infinitive for verbs and noun lemma for plurals. Current hints are present but can be made into bullet “must” statements.
- **Add negative examples for common hallucinations**: Include a short block like “Invalid outputs to avoid: code fences; ‘full sentence’; invented sentences; proper names despite rule #1; gloss missing.” Negative examples often cut hallucination rates materially.
- **Use structured canonicalization responses**: Prefer `generate_json` for both single and batch lemmatization with a fixed shape (e.g., `{"lemma": "<string>"}` or a map of surface→lemma) and an explicit fallback rule to echo the surface form if unsure. This improves determinism and reduces extra tokens.
- **Guardrail on item caps**: Move the max-items directive into both system and user prompts with the actual numeric limit, and add “do not pad with low-value connectors/articles; stop at the cap.” This reduces noisy tail items.
- **Surface verification hints to the model**: Tell the extractor that items failing presence checks will be discarded, so it should only emit spans it can point to. This reduces hallucinated spans before they reach `TextVerifier`.

## Testing & Quality Gaps
- No automated tests found for ingestion, verification, or review flows; regressions (e.g., the review completion bug) would be hard to catch. Add minimal unit tests around ingest pipeline validation/dedup paths and a front-end test that exercises advancing through a full review deck.
- Consider integration smoke tests for `/api/health`, `/api/sources`, and `/api/review/today` to ensure basic wiring (DB + provider) is detected in CI.
