# ============================================
# LiteLLM Proxy Service - API Keys Only
# ============================================
# This service is isolated from the backend database
# NO DATABASE_URL should be set here

# ============================================
# Cloud Provider API Keys (Optional)
# ============================================
# Uncomment and set as needed for cloud fallbacks
# These keys enable LiteLLM to route to cloud providers

# OpenAI (GPT models)
# Cost: ~$0.15-0.60 per 1M tokens depending on model
# OPENAI_API_KEY=sk-...

# Google Gemini
# Free tier: 15 RPM, 1500 RPD
# GEMINI_API_KEY=...

# Anthropic Claude
# Cost: ~$3-15 per 1M tokens depending on model
# ANTHROPIC_API_KEY=sk-ant-...

# HuggingFace Inference API
# Free tier available, pay for higher usage
# HUGGINGFACE_API_KEY=hf_...

# Azure OpenAI (if using Azure)
# AZURE_API_KEY=...
# AZURE_API_BASE=https://....openai.azure.com/
# AZURE_API_VERSION=2023-05-15

# ============================================
# LiteLLM Advanced Features (Optional)
# ============================================

# Master Key for LiteLLM admin API (if you want to use the admin UI)
# LITELLM_MASTER_KEY=sk-1234...

# Redis connection for caching and rate limiting
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_PASSWORD=...

# ============================================
# Local Provider Settings
# ============================================

# Ollama is configured in litellm_config.yaml
# No environment variables needed - Ollama runs on localhost:11434
# Make sure Ollama is running: ollama serve

# LM Studio (if using)
# LM Studio runs on localhost:1234 by default
# No environment variables needed - configure in litellm_config.yaml
