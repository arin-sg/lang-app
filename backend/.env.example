# Database
DATABASE_URL=sqlite:///./data/lang_app.db

# ============================================
# LLM Provider Selection
# ============================================
# Choose which provider to use for each task type
# Options: ollama, litellm, lm_studio, openai, gemini
EXTRACTION_PROVIDER=ollama
EXPLANATION_PROVIDER=ollama

# ============================================
# Ollama (Local LLM - Free, Unlimited)
# ============================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EXTRACTION_MODEL=llama3.2
OLLAMA_EXPLANATION_MODEL=llama3.2

# ============================================
# LiteLLM Service Integration (Optional)
# ============================================
# LiteLLM now runs as a SEPARATE SERVICE (see ../litellm-service/)
#
# To use LiteLLM provider:
# 1. Start the LiteLLM service first:
#    cd ../litellm-service
#    ./start_litellm.sh
#
# 2. Configure backend to use it:
#    EXTRACTION_PROVIDER=litellm
#    EXPLANATION_PROVIDER=litellm
#
# LiteLLM Service Settings:
LITELLM_BASE_URL=http://localhost:4000
LITELLM_EXTRACTION_MODEL=extraction-model
LITELLM_EXPLANATION_MODEL=explanation-model
#
# Note: API keys for cloud providers (OpenAI, Gemini, etc.) are configured
# in the litellm-service/.env file, NOT here. See ../litellm-service/README.md

# API
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
CORS_ORIGINS=http://localhost:5173

# Extraction
EXTRACTION_TIMEOUT_SECONDS=60
MAX_TEXT_LENGTH=10000
