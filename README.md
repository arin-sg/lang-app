# Language Learning App - Iteration 1

An AI-powered personalized German language learning application with local-first architecture.

## Overview

This application helps you learn German by:
- Ingesting German text and extracting learnable items (vocabulary, phrases, patterns)
- Building a personalized knowledge graph of your understanding
- Creating targeted review decks based on your learning history
- Tracking errors to provide adaptive practice

## Technology Stack

- **Backend**: Python 3.10+ with FastAPI
- **Frontend**: React with Vite
- **Database**: SQLite (local-first)
- **LLM**: Multi-provider support via LiteLLM proxy (Ollama, OpenAI, Gemini, HuggingFace, etc.)

## Design System

**Theme**: Cheerful Nanobanana - A warm, friendly aesthetic designed for an enjoyable learning experience.

**UI Framework**:
- **Component Library**: shadcn/ui
- **Styling**: Tailwind CSS v4 with CSS variables
- **Font**: Nunito (Google Fonts)
- **Color Palette**: Warm banana yellow primary, creamy backgrounds
- **Border Radius**: 1.25rem (20px) for bubbly feel

**Key Design Principles**:
- Warm, not cool (browns/golds instead of grays/blues)
- Bubbly and rounded (20px border radius throughout)
- Bold typography (Nunito font-extrabold for headings)
- Playful color accents (banana yellow, pale sky blue)

## Prerequisites

Before you begin, ensure you have:

1. **Python 3.10 or higher**
   ```bash
   python3 --version
   ```

2. **Node.js 18 or higher**
   ```bash
   node --version
   ```

3. **LLM Provider** (choose one):

   **Option A: Ollama (Local, Free, Recommended for Development)**
   ```bash
   # Install Ollama from https://ollama.ai
   ollama --version
   ollama serve

   # Pull a model
   ollama pull llama3.2  # or mistral, qwen, etc.
   ollama list  # Verify model is available
   ```

   **Option B: LiteLLM Proxy (Multi-Provider Gateway)**
   - Supports 100+ LLM providers (Ollama, OpenAI, Gemini, HuggingFace, etc.)
   - See [LiteLLM Setup](#litellm-multi-provider-setup-optional) section below

   **Option C: Cloud Providers (Direct)**
   - OpenAI API key: Set `EXTRACTION_PROVIDER=openai` (coming soon)
   - Gemini API key: Set `EXTRACTION_PROVIDER=gemini` (coming soon)

## Setup Instructions

### Backend Setup

1. Navigate to the backend directory:
   ```bash
   cd backend
   ```

2. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Copy the environment template and configure:
   ```bash
   cp .env.example .env
   # Edit .env if needed (default values should work for local development)
   ```

5. Run database migrations:
   ```bash
   alembic upgrade head
   ```

6. Start the backend server:
   ```bash
   python run.py
   ```

   The API will be available at `http://localhost:8000`

### Frontend Setup

1. Navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Create environment file:
   ```bash
   echo "VITE_API_BASE_URL=http://localhost:8000/api" > .env
   ```

4. Start the development server:
   ```bash
   npm run dev
   ```

   The app will be available at `http://localhost:5173`

## Development Workflow

### Running the Application

#### Basic Setup (Ollama)

You'll need **two terminal windows**:

**Terminal 1 - Backend:**
```bash
cd backend
source venv/bin/activate
python run.py  # Runs on http://localhost:8000
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev  # Runs on http://localhost:5173
```

#### With LiteLLM Service (Optional)

If using the LiteLLM multi-provider gateway, you'll need **three terminal windows**:

**Terminal 1 - LiteLLM Service:**
```bash
cd litellm-service
./start_litellm.sh  # Runs on http://localhost:4000
```

**Terminal 2 - Backend:**
```bash
cd backend
source venv/bin/activate
python run.py  # Configure EXTRACTION_PROVIDER=litellm in .env
```

**Terminal 3 - Frontend:**
```bash
cd frontend
npm run dev
```

### Database Management

**Run migrations:**
```bash
cd backend
alembic upgrade head
```

**Create a new migration:**
```bash
alembic revision -m "description of change"
```

**Inspect the database:**
```bash
cd backend/data
sqlite3 lang_app.db
.tables
.schema items
SELECT * FROM items LIMIT 5;
```

## LiteLLM Multi-Provider Setup (Optional)

LiteLLM provides a unified gateway to 100+ LLM providers with built-in fallbacks, load balancing, and cost tracking.

**IMPORTANT**: LiteLLM now runs as a **separate service** in the `litellm-service/` directory. This isolation prevents environment variable conflicts with the backend database.

### Why Use LiteLLM?

- **Flexibility**: Switch between providers without code changes
- **Reliability**: Automatic fallbacks if primary provider fails
- **Cost Optimization**: Load balance across multiple deployments
- **Unified Interface**: Same API for Ollama, OpenAI, Gemini, HuggingFace, etc.
- **Built-in Features**: Rate limiting, cost tracking, retry logic

### Quick Start

1. **Setup LiteLLM Service**:
   ```bash
   cd litellm-service

   # Create virtual environment
   python3 -m venv venv
   source venv/bin/activate

   # Install dependencies
   pip install -r requirements.txt
   ```

2. **Configure Providers** (optional for cloud providers):
   ```bash
   # Copy .env template if using cloud providers
   cp .env.example .env
   # Edit .env and add API keys (OPENAI_API_KEY, GEMINI_API_KEY, etc.)
   ```

   For local Ollama only, no `.env` configuration needed!

3. **Customize Provider Configuration** in `litellm-service/litellm_config.yaml`:
   ```yaml
   model_list:
     # Use Ollama (local, free) - Default
     - model_name: extraction-model
       litellm_params:
         model: ollama/llama3.2
         api_base: http://localhost:11434

     # Or add OpenAI (cloud, paid)
     # - model_name: extraction-model-openai
     #   litellm_params:
     #     model: openai/gpt-4o-mini
     #     api_key: os.environ/OPENAI_API_KEY
   ```

4. **Start LiteLLM Service**:
   ```bash
   # Terminal 1: Start LiteLLM proxy service
   cd litellm-service
   ./start_litellm.sh  # Runs on port 4000
   ```

5. **Configure Backend** in `backend/.env`:
   ```bash
   EXTRACTION_PROVIDER=litellm
   LITELLM_BASE_URL=http://localhost:4000
   LITELLM_EXTRACTION_MODEL=extraction-model
   ```

6. **Start Backend**:
   ```bash
   # Terminal 2: Start FastAPI backend
   cd backend
   source venv/bin/activate
   python run.py
   ```

**See [litellm-service/README.md](litellm-service/README.md) for comprehensive provider configuration examples, advanced features (fallbacks, load balancing, cost tracking), and troubleshooting.**

### Quick Provider Reference

All provider configuration is in `litellm-service/litellm_config.yaml`. Here are common examples:

**Ollama (Local, Free)**:
```yaml
- model_name: extraction-model
  litellm_params:
    model: ollama/llama3.2
    api_base: http://localhost:11434
```

**OpenAI** (set `OPENAI_API_KEY` in `litellm-service/.env`):
```yaml
- model_name: extraction-model
  litellm_params:
    model: openai/gpt-4o-mini
    api_key: os.environ/OPENAI_API_KEY
```

**Gemini** (set `GEMINI_API_KEY` in `litellm-service/.env`):
```yaml
- model_name: extraction-model
  litellm_params:
    model: gemini/gemini-1.5-flash
    api_key: os.environ/GEMINI_API_KEY
```

**For complete examples** (HuggingFace, LM Studio, Claude, Azure), see [litellm-service/README.md](litellm-service/README.md).

### Switching Providers

To switch providers:

```bash
# 1. Edit litellm-service/litellm_config.yaml
# 2. Restart the LiteLLM service
cd litellm-service
pkill litellm
./start_litellm.sh

# Backend automatically uses new provider (no restart needed)
```

### Backend Configuration

Configure the backend to use LiteLLM in `backend/.env`:

```bash
# Provider Selection
EXTRACTION_PROVIDER=litellm     # Use LiteLLM proxy
EXPLANATION_PROVIDER=litellm    # Both tasks use LiteLLM

# LiteLLM Service Connection
LITELLM_BASE_URL=http://localhost:4000
LITELLM_EXTRACTION_MODEL=extraction-model
LITELLM_EXPLANATION_MODEL=explanation-model
```

**Note**: Cloud provider API keys go in `litellm-service/.env`, NOT `backend/.env`.

# Verify provider is accessible
# For Ollama:
curl http://localhost:11434/api/tags

# For OpenAI:
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

**Model not found:**
```bash
# List available models in LiteLLM
curl http://localhost:4000/models

# Verify model name matches litellm_config.yaml
```

### Cost Warnings

âš ï¸ **Cloud providers are paid services:**
- OpenAI GPT-4o-mini: ~$0.15/1M input tokens, ~$0.60/1M output tokens
- Gemini 1.5 Flash: Free tier 15 RPM, then paid
- Claude 3 Haiku: ~$0.25/1M input tokens, ~$1.25/1M output tokens
- HuggingFace: Free tier available, paid for higher usage

Always set budget limits and monitor usage when using cloud providers.

## Project Structure

```
lang-app/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”‚   â”œâ”€â”€ models/       # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas/      # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic
â”‚   â”‚   â”œâ”€â”€ db/           # Database session
â”‚   â”‚   â””â”€â”€ utils/        # Utilities (Ollama client)
â”‚   â”œâ”€â”€ alembic/          # Database migrations
â”‚   â”œâ”€â”€ data/             # SQLite database files
â”‚   â””â”€â”€ run.py            # Entry point
â”œâ”€â”€ frontend/             # React frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ api/          # Backend API client
â”‚       â”œâ”€â”€ components/   # Reusable components
â”‚       â””â”€â”€ pages/        # Page components
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ tasks.md              # Implementation progress
â””â”€â”€ README.md             # This file
```

## API Endpoints

### Health Check
- `GET /api/health` - Check system health

### Text Ingestion
- `POST /api/sources` - Ingest German text and extract items

### Review
- `GET /api/review/today` - Get today's review deck
- `POST /api/review/result` - Record review result

## Usage Example

1. **Ingest Text**: Paste German text in the Ingest page
2. **View Extraction**: See the items and phrases extracted
3. **Review**: Navigate to Review page to practice
4. **Track Progress**: Your results are saved for personalized learning

## Troubleshooting

**Backend won't start:**
- Check that the virtual environment is activated
- Ensure all dependencies are installed
- Verify Ollama is running: `curl http://localhost:11434/api/tags`

**Frontend can't connect to backend:**
- Verify backend is running on port 8000
- Check CORS settings in `backend/app/main.py`
- Ensure `.env` has correct API URL

**Database errors:**
- Run migrations: `alembic upgrade head`
- Check that `backend/data/` directory exists

## Current Status

**ðŸŽ‰ Iteration 1 - COMPLETE!**

All 6 phases have been successfully implemented and tested:
- âœ… Phase 1: Project Foundation & Setup
- âœ… Phase 2: Database Schema & Models
- âœ… Phase 3: Core Services Layer
- âœ… Phase 4: API Layer
- âœ… Phase 5: Frontend Implementation
- âœ… Phase 6: Integration & Testing

### Completed Components

**Backend:**
- SQLAlchemy models: Items, Edges, Encounters, ErrorTags
- Database migrations with seeded error taxonomy (17 tags)
- Core services: GraphStore, OllamaClient, ExtractService, IngestService, ReviewService
- API endpoints:
  - `POST /api/sources` - Ingest German text and extract learnable items
  - `GET /api/review/today` - Get review deck with least recently seen items
  - `POST /api/review/result` - Record review results with telemetry
  - `GET /api/health` - System health check (database + Ollama)

**Frontend:**
- React Router with 3 pages: Ingest, Review, Coach
- API client with axios for backend communication
- IngestPage: Text input form with extraction results display
- ReviewPage: Interactive review deck with answer submission
- CoachPage: Placeholder for Iteration 2 features
- Responsive layout with navigation

**Database:**
- 4 tables: items, edges, encounters, error_tags
- Graph-shaped data model for knowledge representation
- 17 German grammar error categories seeded
- SQLite with WAL mode for concurrent access

### Test Results

All end-to-end tests passing:
- âœ… Text ingestion successfully extracts items from German text
- âœ… Review deck returns items with complete metadata
- âœ… Review results are recorded with timestamps
- âœ… Data persists correctly in SQLite database
- âœ… Both frontend and backend run without errors

**Ready for**: User acceptance testing and Iteration 2 planning

---

### Iteration 1.5 - UX Enhancements âœ… COMPLETE

**Goal**: Add essential UX features based on user feedback before starting Iteration 2

**New Features**:
1. **Library/Collection View** ðŸ“š
   - Browse all learned items in a filterable list
   - View detailed information for each item
   - See relationships between items (collocations, confusables, etc.)
   - Track learning statistics per item

2. **Flip-Card Review Mode** ðŸŽ´
   - Anki-style flip cards as default review mode
   - Recognition-based learning (see â†’ remember â†’ check)
   - Self-grading with "I knew it" / "I didn't know" buttons
   - Toggle between flip-card and typing modes

**Status**:
- âœ… Backend library API (schemas, endpoints, graph queries)
- âœ… Flip-card review mode (frontend only)
- âœ… Library frontend (page, cards, detail modal)
- âœ… Testing and documentation

See [ITERATION_1_5_PLAN.md](ITERATION_1_5_PLAN.md) for detailed implementation plan.

**What You'll Be Able to Do**:
1. Browse your learning library with filters (All / Words / Phrases / Patterns)
2. Click any item to see:
   - Full metadata (gender, CEFR level, translations)
   - Your performance stats (success rate, times seen)
   - Related words ("often used with", "don't confuse with")
3. Review with flip cards (default) or typing mode (toggle)
4. See your progress at a glance
5. **Select and delete items** with multi-select checkboxes and confirmation dialog

---

### Iteration 1.5.1 - Library Deletion âœ… COMPLETE

**Enhancement**: Multi-select deletion in Library view

**Features**:
- Select individual items or use "Select All"
- Visual feedback for selected items
- Confirmation dialog before deletion
- Cascade deletion (removes encounters and relationships)
- Auto-refresh after deletion

This ensures users can manage their library by removing unwanted items before moving to Iteration 2.

---

### Iteration 1.5.2 - German Verb Phrase Normalization âœ… COMPLETE

**Enhancement**: Improved extraction quality for German verb phrases

**Problem Addressed**: Extracted verb phrases used literal word order ("suchen ein Taxi") instead of dictionary form ("ein Taxi suchen"), which could lead to incorrect learning patterns.

**Solution**:
- Updated extraction prompt with German grammar normalization rules
- Dual-form approach: surface_form (verification) + canonical (learning)
- Added source context display in Library item details
- Examples in prompt: "kaufe ein Buch" â†’ "ein Buch kaufen"

**Impact**: Learners now see correct German dictionary forms for verb phrases, preventing anglicized patterns like "Ich muss suchen ein Taxi" (wrong) vs "Ich muss ein Taxi suchen" (correct).

---

### Iteration 1.5.3 - Smart Extraction Filtering âœ… COMPLETE

**Enhancement**: Intelligent extraction filtering and performance optimization

**Problem Addressed**:
- Extracting low-value items (proper nouns like "Silke", "Berlin")
- Failing to normalize complex verb forms (extracting "kennengelernt" instead of "sich kennenlernen")
- No text length limits causing potential timeouts
- Slow processing for longer texts
- Missing source context in extraction results

**Solution**:
1. **Enhanced Extraction Prompt**:
   - Filter proper nouns (people, cities, countries)
   - Filter simple connectors (und, aber, oder, denn)
   - Advanced verb normalization (participles â†’ infinitives)
   - Reflexive verb handling (include "sich")
   - Separable verb handling (combine prefix + verb)
   - Collocation detection ("Ferien machen" as phrase)
   - Configurable extraction limits (top 5 items per type per batch)

2. **Text Length Enforcement**:
   - 500 character limit with real-time counter
   - Visual warning and disabled button when over limit
   - API validation at backend boundary

3. **Parallel Sentence Batching**:
   - Split text into 2-sentence chunks
   - Process batches in parallel for 2-3x speedup
   - Configurable batch size and item limits
   - Automatic fallback for short texts

4. **Source Context Display**:
   - Show source sentence for each extracted item on IngestPage
   - Consistent italic styling with LibraryPage

**Files Modified**:
- Backend: [extract_service.py](backend/app/services/extract_service.py), [ingest_service.py](backend/app/services/ingest_service.py), [config.py](backend/app/config.py), [.env](backend/.env), [source.py](backend/app/schemas/source.py)
- Frontend: [IngestPage.jsx](frontend/src/pages/IngestPage.jsx)
- Docs: [Feedback.md](docs/Feedback.md), [tasks.md](tasks.md), [CLAUDE.md](CLAUDE.md)

**Impact**:
- Higher extraction quality with intelligent filtering
- Faster processing with parallel batching
- Better user experience with character limits and feedback
- Enhanced learning with source context
- All changes backward compatible and configurable

---

### Iteration 1.5.4 - Extraction Quality & Validation Fixes âœ… COMPLETE

**Enhancement**: Multi-layered validation and quality controls for robust extraction

**Problems Addressed**:
- Blank chunks/words with empty canonical_form in database
- Proper nouns still being extracted despite filtering rules (Silke, Julia, Berlin, UniversitÃ¤t)
- Inconsistent extraction results across similar inputs
- LLM (mistral:latest 7B) not following complex instructions

**Solution** - Defense in Depth:

1. **Post-LLM Validation Layer (CRITICAL)**:
   - `validate_and_filter_items()` function filters invalid items
   - Blank value filtering: rejects items with empty canonical/surface_form
   - Proper noun filtering: 30+ known names, cities, countries (Silke, Julia, Berlin, etc.)
   - Low-value filtering: connectors (und/aber), articles (der/die/das)

2. **Simplified Extraction Prompt**:
   - Reduced from 9 complex rules to 5 clear rules
   - Negative examples first (what NOT to extract)
   - Simpler language optimized for 7B models
   - More explicit about required fields

3. **Enhanced Verification Pipeline**:
   - Additional blank value validation in verification service
   - Tracks invalid items in `verification_stats['invalid']`
   - Double-checks before database storage

4. **Robust Batch Error Handling**:
   - Logs batch failures with index and error type
   - Counts successful vs failed batches
   - Falls back to single-batch extraction if >50% fail
   - Better debugging with detailed error messages

**Files Modified**:
- Backend: [extract_service.py](backend/app/services/extract_service.py), [verification_service.py](backend/app/services/verification_service.py)
- Docs: [Feedback.md](docs/Feedback.md), [tasks.md](tasks.md), [CLAUDE.md](CLAUDE.md)

**Impact**:
- No blank canonical_form values (3 layers of validation)
- Proper nouns consistently filtered (30+ known names)
- Resilient extraction with automatic fallback
- Better debugging with error logs
- Ready for testing with larger LLM models (mistral:7b-instruct-v0.2, llama2:13b)

See [docs/Feedback.md](docs/Feedback.md) for detailed feedback tracking.

---

### Iteration 1.5.5 - Multi-Provider LLM Support âœ… COMPLETE

**Enhancement**: Flexible LLM provider architecture with separate LiteLLM service

**Goal**: Enable easy switching between LLM providers (Ollama, OpenAI, Gemini, HuggingFace, etc.) without code changes

**Architecture**:

```
Backend (Port 8000) â†’ LiteLLM Service (Port 4000) â†’ [Ollama | OpenAI | Gemini | ...]
```

**IMPORTANT**: LiteLLM now runs as a **separate service** to avoid DATABASE_URL environment variable conflicts with the backend database.

---

### Iteration 1.5.6 - Pattern Extraction & UI Terminology âœ… COMPLETE

**Enhancement**: Enable pattern extraction from German text and improve UI terminology

**Problems Addressed**:
- App was not identifying grammatical patterns despite having pattern type support in database
- UI displayed technical term "chunk" instead of user-friendly "Phrase"

**Solutions**:

1. **Pattern Extraction** (Backend):
   - Added Rule 6 to `EXTRACTION_SYSTEM_PROMPT` defining pattern extraction
   - Included pattern example in JSON extraction format
   - Patterns now include: V2 word order, case patterns (mit + Dativ), verb patterns, sentence templates
   - Updated docstring to mention patterns alongside words and chunks

2. **UI Terminology** (Frontend):
   - Updated all UI displays to show "Phrase" instead of "chunk"
   - Backend still uses "chunk" internally (no breaking changes)
   - Applied across IngestPage, LibraryPage, ReviewPage (4 locations)
   - Consistent capitalization with mapping logic

**Files Modified**:
- Backend: [extract_service.py](backend/app/services/extract_service.py) (system prompt, examples, docstring)
- Frontend: [IngestPage.jsx](frontend/src/pages/IngestPage.jsx), [LibraryPage.jsx](frontend/src/pages/LibraryPage.jsx), [ReviewPage.jsx](frontend/src/pages/ReviewPage.jsx)
- Docs: [CLAUDE.md](CLAUDE.md), [README.md](README.md), [tasks.md](tasks.md)

**Impact**:
- Learners can now see and learn German grammatical patterns
- More user-friendly terminology throughout the interface
- Pattern filter in Library view now shows results
- All changes backward compatible

**Pattern Examples**:
- **Word Order**: "Verb-Position-2 (V2)" - Main clause verb in second position
- **Case Patterns**: "mit + Dativ" - Preposition case requirements
- **Verb Patterns**: "modal verb + infinitive" - Ich muss ... gehen
- **Templates**: "Ich mÃ¶chte ... [verb]" - Common sentence structures

---

**Implementation**:

1. **Provider Abstraction Layer**:
   - `LLMProvider` base interface with 5 methods
   - `OllamaProvider` implementation (existing, unchanged)
   - `LiteLLMProvider` implementation (new, OpenAI-compatible)
   - Factory pattern with task-based routing
   - Unified exception hierarchy

2. **Configuration-Driven Provider Selection**:
   - Environment variables: `EXTRACTION_PROVIDER`, `EXPLANATION_PROVIDER`
   - Valid options: `ollama`, `litellm`, `lm_studio`, `openai`, `gemini`
   - Settings for each provider (base URL, model names, timeouts)
   - Backward compatible (defaults to Ollama)

3. **LiteLLM Service (Separate Project)**:
   - New directory: `litellm-service/` at project root
   - Isolated virtual environment with own dependencies
   - Configuration: `litellm_config.yaml` with model routing
   - Supports 100+ providers via unified proxy
   - Built-in features: fallbacks, load balancing, cost tracking, rate limiting
   - No DATABASE_URL conflict (clean environment isolation)

**Files Created**:
- `litellm-service/litellm_config.yaml` (85 lines) - Model routing configuration
- `litellm-service/start_litellm.sh` (28 lines) - Service startup script
- `litellm-service/requirements.txt` (23 lines) - LiteLLM dependencies only
- `litellm-service/.env.example` (60 lines) - API keys template
- `litellm-service/README.md` (400+ lines) - Comprehensive service documentation
- `backend/app/providers/litellm_provider.py` (249 lines) - LiteLLMProvider client

**Files Modified**:
- `backend/app/config.py` (+8 settings fields)
- `backend/app/providers/factory.py` (~50 lines - enum, routing, instantiation)
- `backend/.env.example` (updated with integration notes)
- `backend/requirements.txt` (removed litellm[proxy], kept httpx for client)
- Documentation: `README.md`, `CLAUDE.md`, `tasks.md`

**Usage Examples**:

```bash
# Use Ollama (default) - 2 terminals
EXTRACTION_PROVIDER=ollama
python run.py  # Backend only

# Use LiteLLM service - 3 terminals
EXTRACTION_PROVIDER=litellm

# Terminal 1: Start LiteLLM service
cd litellm-service
./start_litellm.sh  # Port 4000

# Terminal 2: Start backend
cd backend
python run.py  # Port 8000

# Terminal 3: Start frontend
cd frontend
npm run dev  # Port 5173

# Switch providers (edit litellm-service/litellm_config.yaml, restart service)
# No backend restart needed!
```

**Benefits**:
- âœ… Zero breaking changes - all existing code works unchanged
- âœ… Easy provider switching via configuration
- âœ… Built-in fallbacks for reliability
- âœ… Cost tracking and budgets (optional)
- âœ… Load balancing across multiple deployments
- âœ… Future-proof (easy to add Anthropic Claude, Cohere, etc.)
- âœ… **Clean separation** - No DATABASE_URL conflicts
- âœ… **Independent scaling** - Service can run on different machines

**Test Results**:
- âœ… All endpoints working with Ollama provider
- âœ… Provider factory correctly routes based on configuration
- âœ… Error handling validates provider names
- âœ… Backward compatibility maintained
- âœ… Full application tested and operational
- âœ… **LiteLLM service starts without DATABASE_URL errors**
- âœ… **HTTP endpoints responding correctly** (`/health`, `/models`)

**Impact**: Users can now easily switch between local (Ollama, LM Studio) and cloud providers (OpenAI, Gemini, HuggingFace) for optimal cost/quality trade-offs, with clean service isolation preventing environment conflicts.

See [LiteLLM Multi-Provider Setup](#litellm-multi-provider-setup-optional) section for detailed configuration examples and [litellm-service/README.md](litellm-service/README.md) for comprehensive service documentation.

## License

Private project - All rights reserved

## Support

For issues or questions, refer to the [tasks.md](tasks.md) file for implementation progress.
